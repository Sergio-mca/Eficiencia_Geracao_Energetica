# -*- coding: utf-8 -*-
"""DadosEficienciaGeracao.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tP7h60-9cbagSuUdkwk25CKDwxNg7yWF
"""

# ==========================================================
# ðŸ”§ ETAPA 1 â€” InstalaÃ§Ã£o e configuraÃ§Ã£o inicial do PySpark
# ==========================================================
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!pip install pyspark==3.5.1 -q

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, regexp_replace, trim, upper, to_date, year, month,
    when, countDistinct, avg, sum as _sum, round
)

spark = SparkSession.builder.master("local[*]").appName("Tratamento ANEEL").getOrCreate()

# ==========================================================
# ðŸ”§ ETAPA 2 â€” Leitura dos arquivos CSV
# ==========================================================
path1 = "/content/drive/MyDrive/Projeto_ELE_ECO/aneel-empreendi-geracao1.csv"
path2 = "/content/drive/MyDrive/Projeto_ELE_ECO/aneel-empreendi-geracaodiario.csv"

# LÃª ambos os arquivos
df1 = spark.read.csv(path1, header=True, sep=";", encoding="latin1")
df2 = spark.read.csv(path2, header=True, sep=";", encoding="latin1")

# Une os dois conjuntos (caso tenham a mesma estrutura)
df = df1.unionByName(df2)

print(f"Total de linhas combinadas: {df.count():,}")

# ==========================================================
# ðŸ”§ ETAPA 3 â€” Limpeza e padronizaÃ§Ã£o das colunas
# ==========================================================

# Remove espaÃ§os e deixa colunas em formato consistente
df = df.select([trim(col(c)).alias(c.strip()) for c in df.columns])

# Corrige colunas numÃ©ricas (trocando vÃ­rgula por ponto e convertendo para double)
num_cols = [
    "MdaPotenciaOutorgadaKw",
    "MdaPotenciaFiscalizadaKw",
    "MdaGarantiaFisicaKw"
]

for c in num_cols:
    df = df.withColumn(c, regexp_replace(col(c), ",", ".").cast("double"))

# Converte datas
df = df.withColumn("DatEntradaOperacao", to_date("DatEntradaOperacao"))
df = df.withColumn("DatGeracaoConjuntoDados", to_date("DatGeracaoConjuntoDados"))

# Cria colunas de ano e mÃªs
df = df.withColumn("ano", year("DatGeracaoConjuntoDados"))
df = df.withColumn("mes", month("DatGeracaoConjuntoDados"))

# ==========================================================
# ðŸ”§ ETAPA 4 â€” CriaÃ§Ã£o de colunas derivadas e mÃ©tricas
# ==========================================================

# Calcula eficiÃªncia: potÃªncia fiscalizada / potÃªncia outorgada
df = df.withColumn(
    "eficiencia_percent",
    when(col("MdaPotenciaOutorgadaKw") > 0,
         round((col("MdaPotenciaFiscalizadaKw") / col("MdaPotenciaOutorgadaKw")) * 100, 2))
    .otherwise(None)
)

# ==========================================================
# ðŸ”§ ETAPA 5 â€” AgregaÃ§Ãµes por UF, tipo de geraÃ§Ã£o e mÃªs
# ==========================================================

df_agregado = (
    df.groupBy("ano", "mes", "SigUFPrincipal", "SigTipoGeracao")
      .agg(
          countDistinct("NomEmpreendimento").alias("num_empreendimentos"),
          round(avg("MdaPotenciaOutorgadaKw"), 2).alias("pot_media_outorgada_kw"),
          round(avg("MdaPotenciaFiscalizadaKw"), 2).alias("pot_media_fiscalizada_kw"),
          round(avg("eficiencia_percent"), 2).alias("eficiencia_media_percent"),
          round(_sum("MdaPotenciaOutorgadaKw"), 2).alias("pot_total_outorgada_kw"),
          round(_sum("MdaPotenciaFiscalizadaKw"), 2).alias("pot_total_fiscalizada_kw")
      )
      .orderBy("ano", "mes", "SigUFPrincipal", "SigTipoGeracao")
)

# ==========================================================
# ðŸ”§ ETAPA 6 â€” EstatÃ­sticas descritivas (opcional)
# ==========================================================
df_agregado.describe().show(truncate=False)

# ==========================================================
# ðŸ”§ ETAPA 7 â€” ExportaÃ§Ã£o final para CSV (Looker Studio)
# ==========================================================
output_path = "/content/energia_tratada_aneel.csv"
df_agregado.coalesce(1).write.option("header", True).mode("overwrite").csv(output_path)

print(f"\nâœ… Arquivo tratado salvo em: {output_path}")
print("Pronto para importar no Looker Studio!")